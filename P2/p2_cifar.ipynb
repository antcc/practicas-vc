{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p2_cifar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EOH-_j974vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TkW583PDSVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############################################################################\n",
        "# Visión por Computador. Curso 2019/20.\n",
        "# Práctica 2: Redes neuronales convolucionales. Parte 1: conjunto CIFAR100.\n",
        "# Antonio Coín Castro.\n",
        "#############################################################################\n",
        "\n",
        "#\n",
        "# LIBRERÍAS\n",
        "#\n",
        "\n",
        "# Generales\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Keras\n",
        "import keras\n",
        "import keras.utils as np_utils\n",
        "\n",
        "# Tensorflow\n",
        "from tensorflow.compat.v1 import logging\n",
        "\n",
        "# Modelos y capas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Flatten, Activation, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "# Optimizador\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# Función de pérdida\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "# Callbacks\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
        "\n",
        "# Conjunto de datos y preprocesamiento\n",
        "from keras.datasets import cifar100\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#\n",
        "# PARÁMETROS GLOBALES\n",
        "#\n",
        "\n",
        "N = 25                             # Número de clases\n",
        "EPOCHS = 100                       # Épocas de entrenamiento\n",
        "BATCH_SIZE = 64                    # Tamaño de cada batch de imágenes\n",
        "SPLIT = 0.1                        # Partición para validación\n",
        "INPUT_SHAPE = (32, 32, 3)          # Formato de entrada de imágenes\n",
        "PATIENCE = 10                      # Épocas que esperar mientras el modelo no mejora\n",
        "TAM = (10, 5)                      # Tamaño del plot\n",
        "TEMP = False                       # Generar archivos temporales o definitivos\n",
        "ACC_NAME = \"acc\"                   # Nombre de la métrica de precisión\n",
        "DIR = \"/content/drive/My Drive/\"   # Directorio de trabajo\n",
        "\n",
        "#\n",
        "# FUNCIONES AUXILIARES\n",
        "#\n",
        "\n",
        "def wait():\n",
        "    \"\"\"Introduce una espera hasta que se pulse una tecla.\"\"\"\n",
        "\n",
        "    input(\"(Pulsa cualquier tecla para continuar...)\")\n",
        "\n",
        "#\n",
        "# LECTURA Y MODIFICACIÓN DEL CONJUNTO DE IMÁGENES\n",
        "#\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Carga el conjunto de datos en 4 vectores: las imágenes de entrenamiento,\n",
        "       las clases de las imágenes de entrenamiento, las imágenes de test y las\n",
        "       clases de las imágenes de test.\n",
        "\n",
        "       Restringimos el conjunto para que tenga solo N clases. Cada imagen tiene\n",
        "       tamaño (32, 32, 3).\"\"\"\n",
        "\n",
        "    # Leemos y normalizamos los datos\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode = 'fine')\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "\n",
        "    # Nos quedamos con las 25 primeras clases\n",
        "    train_idx = np.isin(y_train, np.arange(N))\n",
        "    train_idx = np.reshape(train_idx, -1)\n",
        "    x_train = x_train[train_idx]\n",
        "    y_train = y_train[train_idx]\n",
        "\n",
        "    test_idx = np.isin(y_test, np.arange(N))\n",
        "    test_idx = np.reshape(test_idx, -1)\n",
        "    x_test = x_test[test_idx]\n",
        "    y_test = y_test[test_idx]\n",
        "\n",
        "    # Convertimos los vectores de clases en matrices binarias\n",
        "    y_train = np_utils.to_categorical(y_train, N)\n",
        "    y_test = np_utils.to_categorical(y_test, N)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "#\n",
        "# GRÁFICAS DE EVOLUCIÓN Y ESTADÍSTICAS\n",
        "#\n",
        "\n",
        "def show_evolution(hist, name):\n",
        "    \"\"\"Pinta dos gráficas: una con la evolución de la función de pérdida\n",
        "       en el conjunto de entrenamiento y en el de validación, y otra con la evolución\n",
        "       del accuracy en el conjunto de entrenamiento y en el de validación.\n",
        "        - hist: historial de entrenamiento del modelo.\n",
        "        - name: nombre del modelo.\"\"\"\n",
        "\n",
        "    # Evolución de las funciones de pérdida\n",
        "    loss = hist.history['loss']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    plt.figure(figsize = TAM)\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.legend([\"Training loss \" + name, \"Validation loss \" + name])\n",
        "    plt.show()\n",
        "\n",
        "    wait()\n",
        "\n",
        "    # Evolución del accuracy\n",
        "    acc = hist.history[ACC_NAME]\n",
        "    val_acc = hist.history[\"val_\" + ACC_NAME]\n",
        "    plt.figure(figsize = TAM)\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.legend([\"Training accuracy \" + name, \"Validation accuracy \" + name])\n",
        "    plt.show()\n",
        "\n",
        "    wait()\n",
        "\n",
        "def show_evolution_val(*hist, names):\n",
        "    \"\"\"Pinta dos gráficas: una con la evolución de la función de pérdida en el conjunto\n",
        "       de validación para todos los modelos, y otra con la evolución del accuracy\n",
        "       en el conjunto de validación para todos los modelos.\n",
        "        - *hist: historiales de entrenamientos de los modelos.\n",
        "        - names: lista de nombres correspondientes a los modelos (en orden).\"\"\"\n",
        "\n",
        "    # Evolución de las funciones de pérdida\n",
        "    plt.figure(figsize = TAM)\n",
        "    for h in hist:\n",
        "        val_loss = h.history['val_loss']\n",
        "        plt.plot(val_loss)\n",
        "\n",
        "    plt.legend([\"Validation loss \" + names[i] for i in range(len(hist))])\n",
        "    plt.show()\n",
        "\n",
        "    wait()\n",
        "\n",
        "    # Evolución del accuracy\n",
        "    plt.figure(figsize = TAM)\n",
        "    for h in hist:\n",
        "        val_acc = h.history[\"val_\" + ACC_NAME]\n",
        "        plt.plot(val_acc)\n",
        "\n",
        "    plt.legend([\"Validation accuracy \" + names[i] for i in range(len(hist))])\n",
        "    plt.show()\n",
        "\n",
        "    wait()\n",
        "\n",
        "def show_stats(score, hist, name, show = True):\n",
        "    \"\"\"Muestra estadísticas de accuracy y loss y gráficas de evolución.\n",
        "        - score: métricas de evaluación.\n",
        "        - hist: historial de entrenamiento.\n",
        "        - name: nombre del modelo.\n",
        "        - show: controla si se muestran gráficas con estadísticas.\"\"\"\n",
        "\n",
        "    print(\"\\n---------- \" + name.upper() + \" MODEL EVALUATION ----------\")\n",
        "    print(\"Test loss:\", score[0])\n",
        "    print(\"Test accuracy:\", score[1])\n",
        "    print()\n",
        "\n",
        "    # Mostramos gráficas\n",
        "    if show:\n",
        "        show_evolution(hist, name)\n",
        "\n",
        "def show_stats_val(*stats, names, show = True):\n",
        "    \"\"\"Muestra estadísticas de accuracy y loss y gráficas de evolución\n",
        "       en validación.\n",
        "        - *stats: estadísticas de los modelos. Para cada modelo, se\n",
        "          consideran sus métricas de evaluación (score) y su historial debug\n",
        "          entrenamiento. Se mandan primero todos los scores en orden, y\n",
        "          después todos los historiales.\n",
        "        - names: lista de nombres correspondientes a los modelos (en orden).\n",
        "        - show: controla si se muestran gráficas con estadísticas.\"\"\"\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "        print(\"\\n---------- \" + name.upper() + \" MODEL EVALUATION ----------\")\n",
        "        print(\"Test loss:\", stats[i][0])\n",
        "        print(\"Test accuracy:\", stats[i][1])\n",
        "    print()\n",
        "\n",
        "    # Mostramos gráficas\n",
        "    if show:\n",
        "        show_evolution_val(*stats[len(names):], names = names)\n",
        "\n",
        "#\n",
        "# COMPILACIÓN DEL MODELO\n",
        "#\n",
        "\n",
        "def compile(model):\n",
        "    \"\"\"Definición del optimizador y compilación del modelo.\"\"\"\n",
        "\n",
        "    # Definimos el optimizador\n",
        "    opt = SGD(lr = 0.01, decay = 1e-6,\n",
        "              momentum = 0.9, nesterov = True)\n",
        "\n",
        "    # Compilamos el modelo\n",
        "    model.compile(loss = categorical_crossentropy,\n",
        "                  optimizer = opt,\n",
        "                  metrics = [ACC_NAME])\n",
        "\n",
        "#\n",
        "# ENTRENAMIENTO DEL MODELO\n",
        "#\n",
        "\n",
        "def train(model, model_name, datagen, x_train, y_train,\n",
        "          save_hist = False, save_weights = False):\n",
        "    \"\"\"Entrenar el modelo con los datos de entrenamiento. Devuelve\n",
        "       el historial de entrenamiento.\n",
        "        - model, model_name: modelo a entrenar y su nombre.\n",
        "        - datagen: generador de imágenes de entrenamiento y validación.\n",
        "        - x_train, y_train: datos de entrenamiento.\n",
        "        - save_hist: controla si se guarda en fichero el historial de entrenamiento.\n",
        "        - save_weights: controla si se guardan los mejores pesos obtenidos en fichero.\"\"\"\n",
        "\n",
        "    # Nombres de ficheros para guardar estadísticas\n",
        "    if TEMP:\n",
        "        file_w = DIR + \"temp_\" + model_name + \"_weights.h5\"\n",
        "        file_h = DIR + \"temp_\" + model_name + \"_hist\"\n",
        "    else:\n",
        "        file_w = DIR + model_name + \"_weights.h5\"\n",
        "        file_h = DIR + model_name + \"_hist\"\n",
        "\n",
        "    # Callbacks para el entrenamiento\n",
        "    callbacks_list = []\n",
        "\n",
        "    # Paramos si no mejoramos en un número determinado de épocas\n",
        "    early_stopping_loss = EarlyStopping(monitor = 'val_loss',\n",
        "                                        patience = PATIENCE,\n",
        "                                        restore_best_weights = True)\n",
        "    early_stopping_val = EarlyStopping(monitor = \"val_\" + ACC_NAME,\n",
        "                                       patience = PATIENCE,\n",
        "                                       restore_best_weights = True)\n",
        "    \n",
        "    callbacks_list.append(early_stopping_loss)\n",
        "    callbacks_list.append(early_stopping_val)\n",
        "\n",
        "    if save_weights:\n",
        "        # Vamos guardando los mejores pesos del modelo\n",
        "        checkpointer = ModelCheckpoint(monitor = \"val_\" + ACC_NAME,\n",
        "                                       filepath = file_w,\n",
        "                                       verbose = 1,\n",
        "                                       save_weights_only = True,\n",
        "                                       save_best_only = True)\n",
        "        callbacks_list.append(checkpointer)\n",
        "\n",
        "    # Entrenamos el modelo\n",
        "    hist = model.fit_generator(datagen.flow(x_train,\n",
        "                                            y_train,\n",
        "                                            batch_size = BATCH_SIZE,\n",
        "                                            subset = 'training'),\n",
        "                               epochs = EPOCHS,\n",
        "                               steps_per_epoch = len(x_train) * (1 - SPLIT) / BATCH_SIZE,\n",
        "                               verbose = 1,\n",
        "                               validation_data = datagen.flow(x_train,\n",
        "                                                              y_train,\n",
        "                                                              batch_size = BATCH_SIZE,\n",
        "                                                              subset = 'validation'),\n",
        "                               validation_steps = len(x_train) * SPLIT / BATCH_SIZE,\n",
        "                               callbacks = callbacks_list)\n",
        "\n",
        "    # Información del entrenamiento\n",
        "    best_epoch = len(hist.epoch) - PATIENCE\n",
        "    print(\"\\nNo se ha mejorado en \" + str(PATIENCE) + \" épocas.\")\n",
        "    print(\"Mejores pesos obtenidos en la época\", best_epoch)\n",
        "\n",
        "    # Guardamos historial de entrenamiento\n",
        "    if save_hist:\n",
        "        with open(file_h, 'wb') as f:\n",
        "            pickle.dump(hist, f)\n",
        "\n",
        "    return hist\n",
        "\n",
        "#\n",
        "# EVALUACIÓN SOBRE EL CONJUNTO DE TEST\n",
        "#\n",
        "\n",
        "def evaluate(model, x_test, y_test):\n",
        "    \"\"\"Evaluar el modelo sobre el conjunto de test.\n",
        "        - model: modelo a usar para evaluar.\n",
        "        - x_test, y_test: datos de test.\"\"\"\n",
        "\n",
        "    score = model.evaluate(x_test, y_test, verbose = 0)\n",
        "\n",
        "    return score\n",
        "\n",
        "#\n",
        "# EJECUCIÓN COMPLETA DEL MODELO\n",
        "#\n",
        "\n",
        "def execute(model_gen, preproc,\n",
        "            save_s = False, save_h = False, load_w = False, save_w = False):\n",
        "    \"\"\"Construir, compilar, entrenar y evaluar un modelo. Devuelve el\n",
        "       historial de entrenamiento y la evaluación del modelo.\n",
        "        - model_gen: función que devuelve el modelo en cuestión y su nombre.\n",
        "        - preproc: decide si se realiza preprocesamiento de datos.\n",
        "        - save_s: controla si se guardan las estadísticas de evaluación en un fichero.\n",
        "        - save_h: controla si se guarda el historial de entrenamiento en un fichero.\n",
        "        - load_w: controla si se cargan los pesos iniciales de un fichero.\n",
        "        - save_w: controla si se guardan los pesos aprendidos en un fichero.\"\"\"\n",
        "\n",
        "    # Construimos y compilamos el modelo\n",
        "    model, model_name = model_gen()\n",
        "    compile(model)\n",
        "\n",
        "    # Nombres de los ficheros para guardar/cargar\n",
        "    file_w = DIR + model_name + \"_weights.h5\"\n",
        "    if TEMP:\n",
        "        file_s = DIR + \"temp_\" + model_name + \"_score\"\n",
        "    else:\n",
        "        file_s = DIR + model_name + \"_score\"\n",
        "\n",
        "    # Mostramos el modelo\n",
        "    print(model.summary())\n",
        "\n",
        "    # Cargamos los datos\n",
        "    x_train, y_train, x_test, y_test = load_data()\n",
        "\n",
        "    if preproc:\n",
        "        # Generador para las imágenes con preprocesamiento y data augmentation\n",
        "        datagen = ImageDataGenerator(featurewise_center = True,\n",
        "                                     featurewise_std_normalization = True,\n",
        "                                     width_shift_range = 0.1,\n",
        "                                     height_shift_range = 0.1,\n",
        "                                     zoom_range = 0.2,\n",
        "                                     horizontal_flip = True,\n",
        "                                     validation_split = SPLIT)\n",
        "\n",
        "        # Estandarizamos datos de entrenamiento y test\n",
        "        datagen.fit(x_train)\n",
        "        datagen.standardize(x_test)\n",
        "\n",
        "    else:\n",
        "        # Generador para las imágenes sin preprocesamiento\n",
        "        datagen = ImageDataGenerator(validation_split = SPLIT)\n",
        "\n",
        "    # Cargamos pesos precalculados\n",
        "    if load_w:\n",
        "        print(\"\\nCargando pesos precalculados de \" + file_w)\n",
        "        model.load_weights(file_w)\n",
        "\n",
        "    # Entrenamos el modelo\n",
        "    hist = train(model, model_name, datagen, x_train, y_train, save_h, save_w)\n",
        "\n",
        "    # Evaluamos el modelo\n",
        "    score = evaluate(model, x_test, y_test)\n",
        "\n",
        "    # Guardamos el resultado de la evaluación\n",
        "    if save_s:\n",
        "        with open(file_s, 'wb') as f:\n",
        "            pickle.dump(score, f)\n",
        "\n",
        "    return score, hist\n",
        "\n",
        "#\n",
        "# APARTADO 1: BASENET\n",
        "#\n",
        "\n",
        "def basenet_model():\n",
        "    \"\"\"Devuelve el modelo de referencia BaseNet.\"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(6,\n",
        "                     kernel_size = (5, 5),\n",
        "                     activation = 'relu',\n",
        "                     input_shape = INPUT_SHAPE))\n",
        "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    model.add(Conv2D(16,\n",
        "                     kernel_size = (5, 5),\n",
        "                     activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50,\n",
        "                    activation = 'relu'))\n",
        "    model.add(Dense(N,\n",
        "                    activation = 'softmax'))\n",
        "\n",
        "    return model, \"basenet\"\n",
        "\n",
        "def ex1(show = True):\n",
        "    \"\"\"Ejercicio 1 de la práctica 2. Entrenamiento y evaluación sobre\n",
        "       CIFAR100 con la red BaseNet.\n",
        "        - show: controla si se muestran gráficas con estadísticas.\"\"\"\n",
        "\n",
        "    # Ejecutamos el modelo\n",
        "    score, hist = execute(basenet_model, preproc = False)\n",
        "\n",
        "    # Mostramos estadísticas\n",
        "    show_stats(score, hist, \"basenet\", show = show)\n",
        "\n",
        "#\n",
        "# APARTADO 2: BASENET MEJORADO\n",
        "#\n",
        "\n",
        "def improved_basenet_model():\n",
        "    \"\"\"Devuelve el modelo BaseNet mejorado.\"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3),\n",
        "                     use_bias = False,\n",
        "                     input_shape = INPUT_SHAPE))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(32,\n",
        "                     kernel_size = (3, 3),\n",
        "                     use_bias = False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3),\n",
        "                     use_bias = False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64,\n",
        "                     kernel_size = (3, 3),\n",
        "                     use_bias = False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512,\n",
        "                    use_bias = False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(256,\n",
        "                    use_bias = False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(N,\n",
        "                    activation = 'softmax'))\n",
        "\n",
        "    return model, \"improved_basenet\"\n",
        "\n",
        "def ex2(show = True):\n",
        "    \"\"\"Ejercicio 2 de la práctica 2. Entrenamiento y evaluación sobre\n",
        "       CIFAR100 con la red BaseNet mejorada.\n",
        "        - show: controla si se muestran gráficas con estadísticas.\"\"\"\n",
        "\n",
        "    # Ejecutamos el modelo\n",
        "    score, hist = execute(improved_basenet_model, preproc = True)\n",
        "\n",
        "    # Mostramos estadísticas\n",
        "    show_stats(score, hist, \"improved_basenet\", show = show)\n",
        "\n",
        "def compare(show = True):\n",
        "    \"\"\"Comparar el modelo BaseNet con el modelo BaseNet mejorado.\n",
        "        - show: controla si se muestran gráficas con estadísticas.\"\"\"\n",
        "\n",
        "    # Cargamos el historial de entrenamiento de BaseNet\n",
        "    with open(DIR + \"basenet_hist\", 'rb') as f1:\n",
        "        h1 = pickle.load(f1)\n",
        "\n",
        "    # Cargamos las estadísticas de evaluación de BaseNet\n",
        "    with open(DIR + \"basenet_score\", 'rb') as f1:\n",
        "        s1 = pickle.load(f1)\n",
        "\n",
        "    # Cargamos el historial de entrenamiento de BaseNet mejorado\n",
        "    with open(DIR + \"improved_basenet_hist\", 'rb') as f2:\n",
        "        h2 = pickle.load(f2)\n",
        "\n",
        "    # Cargamos las estadísticas de evaluación de BaseNet mejorado\n",
        "    with open(DIR + \"improved_basenet_score\", 'rb') as f2:\n",
        "        s2 = pickle.load(f2)\n",
        "\n",
        "    # Mostramos estadísticas en validación\n",
        "    show_stats_val(s1, s2, h1, h2,\n",
        "                   names = [\"basenet\", \"improved_basenet\"], show = show)\n",
        "\n",
        "#\n",
        "# FUNCIÓN PRINCIPAL\n",
        "#\n",
        "\n",
        "def main():\n",
        "    \"\"\"Ejecuta la primera parte de la práctica 2 paso a paso.\"\"\"\n",
        "\n",
        "    # No mostrar warnings de TensorFlow\n",
        "    logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "    print(\"\\n--- EJERCICIO 1: BASENET ---\\n\")\n",
        "    ex1()\n",
        "\n",
        "    print(\"\\n--- EJERCICIO 2: BASENET MEJORADO ---\\n\")\n",
        "    ex2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dqrvDwlFMDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ejecutar la primera parte de la práctica\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}